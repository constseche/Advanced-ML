{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5829a4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conatseche/PycharmProjects/pythonProject1/venv/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[390  21   4]\n",
      " [ 21  46   4]\n",
      " [  3   7  36]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.94      0.94       414\n",
      "         2.0       0.65      0.62      0.63        74\n",
      "         3.0       0.78      0.82      0.80        44\n",
      "\n",
      "    accuracy                           0.89       532\n",
      "   macro avg       0.79      0.79      0.79       532\n",
      "weighted avg       0.89      0.89      0.89       532\n",
      "\n",
      "SVC loss: 139\n",
      "[[405  21   3]\n",
      " [  8  53   1]\n",
      " [  1   0  40]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.98      0.96       414\n",
      "         2.0       0.85      0.72      0.78        74\n",
      "         3.0       0.98      0.91      0.94        44\n",
      "\n",
      "    accuracy                           0.94       532\n",
      "   macro avg       0.92      0.87      0.89       532\n",
      "weighted avg       0.93      0.94      0.93       532\n",
      "\n",
      "Random Forest loss: 109\n",
      "[[272   1   0]\n",
      " [125  69  17]\n",
      " [ 17   4  27]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      0.66      0.79       414\n",
      "         2.0       0.33      0.93      0.48        74\n",
      "         3.0       0.56      0.61      0.59        44\n",
      "\n",
      "    accuracy                           0.69       532\n",
      "   macro avg       0.63      0.73      0.62       532\n",
      "weighted avg       0.87      0.69      0.73       532\n",
      "\n",
      "Naive Bayes loss: 167\n",
      "[139, 109, 167]\n",
      "==========Undersampling==========\n",
      "Counter({2.0: 221, 1.0: 200, 3.0: 132})\n",
      "[[344   3   1]\n",
      " [ 54  63   6]\n",
      " [ 16   8  37]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.99      0.83      0.90       414\n",
      "         2.0       0.51      0.85      0.64        74\n",
      "         3.0       0.61      0.84      0.70        44\n",
      "\n",
      "    accuracy                           0.83       532\n",
      "   macro avg       0.70      0.84      0.75       532\n",
      "weighted avg       0.89      0.83      0.85       532\n",
      "\n",
      "Linear SVM 101\n",
      "\n",
      "==========Undersampling==========\n",
      "Counter({2.0: 221, 1.0: 200, 3.0: 132})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conatseche/PycharmProjects/pythonProject1/venv/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[369   4   1]\n",
      " [ 37  69   1]\n",
      " [  8   1  42]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.99      0.89      0.94       414\n",
      "         2.0       0.64      0.93      0.76        74\n",
      "         3.0       0.82      0.95      0.88        44\n",
      "\n",
      "    accuracy                           0.90       532\n",
      "   macro avg       0.82      0.93      0.86       532\n",
      "weighted avg       0.93      0.90      0.91       532\n",
      "\n",
      "Random forest 68\n",
      "\n",
      "==========Undersampling==========\n",
      "Counter({2.0: 221, 1.0: 200, 3.0: 132})\n",
      "[[333   8   2]\n",
      " [ 75  63  16]\n",
      " [  6   3  26]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.97      0.80      0.88       414\n",
      "         2.0       0.41      0.85      0.55        74\n",
      "         3.0       0.74      0.59      0.66        44\n",
      "\n",
      "    accuracy                           0.79       532\n",
      "   macro avg       0.71      0.75      0.70       532\n",
      "weighted avg       0.87      0.79      0.82       532\n",
      "\n",
      "Naive Bayes 142\n",
      "\n",
      "==========Oversampling==========\n",
      "Counter({1.0: 1241, 3.0: 1200, 2.0: 1000})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conatseche/PycharmProjects/pythonProject1/venv/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[349   7   1]\n",
      " [ 48  55   5]\n",
      " [ 17  12  38]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.98      0.84      0.91       414\n",
      "         2.0       0.51      0.74      0.60        74\n",
      "         3.0       0.57      0.86      0.68        44\n",
      "\n",
      "    accuracy                           0.83       532\n",
      "   macro avg       0.68      0.82      0.73       532\n",
      "weighted avg       0.88      0.83      0.85       532\n",
      "\n",
      "Linear SVM 115\n",
      "\n",
      "==========Oversampling==========\n",
      "Counter({1.0: 1241, 3.0: 1200, 2.0: 1000})\n",
      "[[400  14   2]\n",
      " [ 12  60   1]\n",
      " [  2   0  41]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.96      0.97      0.96       414\n",
      "         2.0       0.82      0.81      0.82        74\n",
      "         3.0       0.95      0.93      0.94        44\n",
      "\n",
      "    accuracy                           0.94       532\n",
      "   macro avg       0.91      0.90      0.91       532\n",
      "weighted avg       0.94      0.94      0.94       532\n",
      "\n",
      "Random forest 81\n",
      "\n",
      "==========Oversampling==========\n",
      "Counter({1.0: 1241, 3.0: 1200, 2.0: 1000})\n",
      "[[255   1   0]\n",
      " [139  69  17]\n",
      " [ 20   4  27]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      0.62      0.76       414\n",
      "         2.0       0.31      0.93      0.46        74\n",
      "         3.0       0.53      0.61      0.57        44\n",
      "\n",
      "    accuracy                           0.66       532\n",
      "   macro avg       0.61      0.72      0.60       532\n",
      "weighted avg       0.86      0.66      0.70       532\n",
      "\n",
      "Naive Bayes 184\n",
      "\n",
      "==========Combination============\n",
      "Counter({3.0: 1200, 2.0: 1000, 1.0: 200})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conatseche/PycharmProjects/pythonProject1/venv/lib/python3.7/site-packages/imblearn/utils/_validation.py:300: UserWarning: After over-sampling, the number of samples (1000) in class 2 will be larger than the number of samples in the majority class (class #2.0 -> 221)\n",
      "  f\"After over-sampling, the number of samples ({n_samples})\"\n",
      "/Users/conatseche/PycharmProjects/pythonProject1/venv/lib/python3.7/site-packages/imblearn/utils/_validation.py:300: UserWarning: After over-sampling, the number of samples (1200) in class 3 will be larger than the number of samples in the majority class (class #2.0 -> 221)\n",
      "  f\"After over-sampling, the number of samples ({n_samples})\"\n",
      "/Users/conatseche/PycharmProjects/pythonProject1/venv/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/conatseche/PycharmProjects/pythonProject1/venv/lib/python3.7/site-packages/imblearn/utils/_validation.py:300: UserWarning: After over-sampling, the number of samples (1000) in class 2 will be larger than the number of samples in the majority class (class #2.0 -> 221)\n",
      "  f\"After over-sampling, the number of samples ({n_samples})\"\n",
      "/Users/conatseche/PycharmProjects/pythonProject1/venv/lib/python3.7/site-packages/imblearn/utils/_validation.py:300: UserWarning: After over-sampling, the number of samples (1200) in class 3 will be larger than the number of samples in the majority class (class #2.0 -> 221)\n",
      "  f\"After over-sampling, the number of samples ({n_samples})\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[271   0   0]\n",
      " [116  58   4]\n",
      " [ 27  16  40]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      0.65      0.79       414\n",
      "         2.0       0.33      0.78      0.46        74\n",
      "         3.0       0.48      0.91      0.63        44\n",
      "\n",
      "    accuracy                           0.69       532\n",
      "   macro avg       0.60      0.78      0.63       532\n",
      "weighted avg       0.86      0.69      0.73       532\n",
      "\n",
      "Linear SVM 163\n",
      "\n",
      "==========Combination============\n",
      "Counter({3.0: 1200, 2.0: 1000, 1.0: 200})\n",
      "[[355   5   1]\n",
      " [ 49  68   1]\n",
      " [ 10   1  42]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.98      0.86      0.92       414\n",
      "         2.0       0.58      0.92      0.71        74\n",
      "         3.0       0.79      0.95      0.87        44\n",
      "\n",
      "    accuracy                           0.87       532\n",
      "   macro avg       0.78      0.91      0.83       532\n",
      "weighted avg       0.91      0.87      0.88       532\n",
      "\n",
      "Random forest 86\n",
      "\n",
      "==========Combination============\n",
      "Counter({3.0: 1200, 2.0: 1000, 1.0: 200})\n",
      "[[321   4   2]\n",
      " [ 86  67  16]\n",
      " [  7   3  26]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.98      0.78      0.87       414\n",
      "         2.0       0.40      0.91      0.55        74\n",
      "         3.0       0.72      0.59      0.65        44\n",
      "\n",
      "    accuracy                           0.78       532\n",
      "   macro avg       0.70      0.76      0.69       532\n",
      "weighted avg       0.88      0.78      0.80       532\n",
      "\n",
      "Naive Bayes 138\n",
      "\n",
      "==========SVC with class weighting==========\n",
      "[[352   3   0]\n",
      " [ 47  60   5]\n",
      " [ 15  11  39]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.99      0.85      0.92       414\n",
      "         2.0       0.54      0.81      0.65        74\n",
      "         3.0       0.60      0.89      0.72        44\n",
      "\n",
      "    accuracy                           0.85       532\n",
      "   macro avg       0.71      0.85      0.76       532\n",
      "weighted avg       0.90      0.85      0.86       532\n",
      "\n",
      "loss: 90\n",
      "==========Random Forest with class weighting==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conatseche/PycharmProjects/pythonProject1/venv/lib/python3.7/site-packages/imblearn/utils/_validation.py:300: UserWarning: After over-sampling, the number of samples (1000) in class 2 will be larger than the number of samples in the majority class (class #2.0 -> 221)\n",
      "  f\"After over-sampling, the number of samples ({n_samples})\"\n",
      "/Users/conatseche/PycharmProjects/pythonProject1/venv/lib/python3.7/site-packages/imblearn/utils/_validation.py:300: UserWarning: After over-sampling, the number of samples (1200) in class 3 will be larger than the number of samples in the majority class (class #2.0 -> 221)\n",
      "  f\"After over-sampling, the number of samples ({n_samples})\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[403  18   3]\n",
      " [  7  56   1]\n",
      " [  4   0  40]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.95      0.97      0.96       414\n",
      "         2.0       0.88      0.76      0.81        74\n",
      "         3.0       0.91      0.91      0.91        44\n",
      "\n",
      "    accuracy                           0.94       532\n",
      "   macro avg       0.91      0.88      0.89       532\n",
      "weighted avg       0.94      0.94      0.94       532\n",
      "\n",
      "loss: 99\n",
      "==========Naive Bayes with class weighting==========\n",
      "[[249   0   0]\n",
      " [145  70  17]\n",
      " [ 20   4  27]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      0.60      0.75       414\n",
      "         2.0       0.30      0.95      0.46        74\n",
      "         3.0       0.53      0.61      0.57        44\n",
      "\n",
      "    accuracy                           0.65       532\n",
      "   macro avg       0.61      0.72      0.59       532\n",
      "weighted avg       0.86      0.65      0.70       532\n",
      "\n",
      "loss: 186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conatseche/PycharmProjects/pythonProject1/venv/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/conatseche/PycharmProjects/pythonProject1/venv/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/conatseche/PycharmProjects/pythonProject1/venv/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/conatseche/PycharmProjects/pythonProject1/venv/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/conatseche/PycharmProjects/pythonProject1/venv/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/conatseche/PycharmProjects/pythonProject1/venv/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/conatseche/PycharmProjects/pythonProject1/venv/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/conatseche/PycharmProjects/pythonProject1/venv/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/conatseche/PycharmProjects/pythonProject1/venv/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/conatseche/PycharmProjects/pythonProject1/venv/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========SVC==========\n",
      "[[363  12   2]\n",
      " [ 37  53   6]\n",
      " [ 14   9  36]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.96      0.88      0.92       414\n",
      "         2.0       0.55      0.72      0.62        74\n",
      "         3.0       0.61      0.82      0.70        44\n",
      "\n",
      "    accuracy                           0.85       532\n",
      "   macro avg       0.71      0.80      0.75       532\n",
      "weighted avg       0.88      0.85      0.86       532\n",
      "\n",
      "SVC with rejection sampling-votting: 124\n",
      "==========Random Forest==========\n",
      "[[387  11   3]\n",
      " [ 20  62   1]\n",
      " [  7   1  40]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.97      0.93      0.95       414\n",
      "         2.0       0.75      0.84      0.79        74\n",
      "         3.0       0.83      0.91      0.87        44\n",
      "\n",
      "    accuracy                           0.92       532\n",
      "   macro avg       0.85      0.89      0.87       532\n",
      "weighted avg       0.92      0.92      0.92       532\n",
      "\n",
      "Random Forest with rejection sampling-votting: 88\n",
      "==========Naive Bayes==========\n",
      "[[336   8   2]\n",
      " [ 69  63  16]\n",
      " [  9   3  26]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.97      0.81      0.88       414\n",
      "         2.0       0.43      0.85      0.57        74\n",
      "         3.0       0.68      0.59      0.63        44\n",
      "\n",
      "    accuracy                           0.80       532\n",
      "   macro avg       0.69      0.75      0.70       532\n",
      "weighted avg       0.87      0.80      0.82       532\n",
      "\n",
      "Naive Bayes with rejection sampling: 139\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_ticks() got an unexpected keyword argument 'color'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-59bb91e93593>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mcost_sensitive_learning\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m# from sklearn.datasets import load_breast_cancer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Advanced-ML/cost_sensitive_learning.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    370\u001B[0m \u001B[0;31m# print(df2)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    371\u001B[0m \u001B[0mtolist\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 372\u001B[0;31m \u001B[0mgrouped_bar\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtolist\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/PycharmProjects/Advanced-ML/cost_sensitive_learning.py\u001B[0m in \u001B[0;36mgrouped_bar\u001B[0;34m(loss_arr)\u001B[0m\n\u001B[1;32m    114\u001B[0m         \u001B[0mbars\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0max\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbar\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mPos\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mi\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mwidth\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss_arr\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwidth\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mwidth\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmethods\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 116\u001B[0;31m     \u001B[0max\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_xticks\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mPos\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mwidth\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0;36m4\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcolor\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'white'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    117\u001B[0m     \u001B[0max\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_xticklabels\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[0max\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbar_label\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbars\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpadding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pythonProject1/venv/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     72\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 73\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mget_method\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     74\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     75\u001B[0m         \u001B[0mwrapper\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__module__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mowner\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__module__\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: set_ticks() got an unexpected keyword argument 'color'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cost_sensitive_learning\n",
    "\n",
    "\n",
    "# from sklearn.datasets import load_breast_cancer\n",
    "# data = load_breast_cancer()\n",
    "# list(data.target_names) # 0 is malignant, 1 is benign\n",
    "# frame = pd.DataFrame(data.target, columns=['target'])\n",
    "# frame['target'].value_counts()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from costcla.metrics import cost_loss\n",
    "\n",
    "data = pd.read_csv(\"data/fetal_health.csv\")\n",
    "X_train, X_test, y_train, y_test = cost_sensitive_learning.pre_processing_binary(data)\n",
    "\n",
    "#fp, fn, tp, tn\n",
    "# create an example-dependent cost-matrix required by costclas\n",
    "fp = np.full((y_test.shape[0],1), 1)\n",
    "fn = np.full((y_test.shape[0],1), 4)\n",
    "tp = np.zeros((y_test.shape[0],1))\n",
    "tn = np.zeros((y_test.shape[0],1))\n",
    "cost_matrix = np.hstack((fp, fn, tp, tn))\n",
    "\n",
    "# create a classic cost-matrix\n",
    "cost_m = [[0 , 4], [1, 0]]\n",
    "\n",
    "names = ['random forest', 'linear SVM']\n",
    "classifiers = [RandomForestClassifier(n_estimators=100, random_state=0),\n",
    "               SVC(kernel='linear', C=1)]\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "  print(name)\n",
    "  clf.fit(X_train, y_train)\n",
    "  y_pred = clf.predict(X_test)\n",
    "  print(classification_report(y_test, y_pred))\n",
    "\n",
    "  conf_m = confusion_matrix(y_test, y_pred).T # transpose to align with slides\n",
    "  print(conf_m)\n",
    "  print(np.sum(conf_m * cost_m))\n",
    "  loss = cost_loss(y_test, y_pred, cost_matrix)\n",
    "  print(\"%d\\n\" %loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55b6f132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no cost minimization\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.96       414\n",
      "           1       0.89      0.79      0.84       118\n",
      "\n",
      "    accuracy                           0.93       532\n",
      "   macro avg       0.92      0.88      0.90       532\n",
      "weighted avg       0.93      0.93      0.93       532\n",
      "\n",
      "111\n",
      "\n",
      "[[403  25]\n",
      " [ 11  93]]\n",
      "no calibration\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95       414\n",
      "           1       0.78      0.93      0.85       118\n",
      "\n",
      "    accuracy                           0.93       532\n",
      "   macro avg       0.88      0.93      0.90       532\n",
      "weighted avg       0.94      0.93      0.93       532\n",
      "\n",
      "63\n",
      "\n",
      "[[383   8]\n",
      " [ 31 110]]\n",
      "costcla calibration on training set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95       414\n",
      "           1       0.91      0.75      0.82       118\n",
      "\n",
      "    accuracy                           0.93       532\n",
      "   macro avg       0.92      0.86      0.89       532\n",
      "weighted avg       0.93      0.93      0.92       532\n",
      "\n",
      "129\n",
      "\n",
      "[[405  30]\n",
      " [  9  88]]\n",
      "\n",
      "sigmoid calibration\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95       414\n",
      "           1       0.80      0.90      0.84       118\n",
      "\n",
      "    accuracy                           0.93       532\n",
      "   macro avg       0.88      0.92      0.90       532\n",
      "weighted avg       0.93      0.93      0.93       532\n",
      "\n",
      "75\n",
      "\n",
      "[[387  12]\n",
      " [ 27 106]]\n",
      "\n",
      "isotonic calibration\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95       414\n",
      "           1       0.78      0.93      0.85       118\n",
      "\n",
      "    accuracy                           0.93       532\n",
      "   macro avg       0.88      0.93      0.90       532\n",
      "weighted avg       0.94      0.93      0.93       532\n",
      "\n",
      "63\n",
      "\n",
      "[[383   8]\n",
      " [ 31 110]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from costcla.models import BayesMinimumRiskClassifier\n",
    "\n",
    "cost_min_loss = []\n",
    "print(\"no cost minimization\")\n",
    "clf = RandomForestClassifier(random_state=0, n_estimators=100)\n",
    "model = clf.fit(X_train, y_train)\n",
    "pred_test = model.predict(X_test)\n",
    "print(classification_report(y_test, pred_test))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "cost_min_loss.append(loss)\n",
    "print(\"%d\\n\" %loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) # transpose to align with slides\n",
    "\n",
    "\n",
    "print(\"no calibration\")\n",
    "clf = RandomForestClassifier(random_state=0, n_estimators=100)\n",
    "model = clf.fit(X_train, y_train)\n",
    "prob_test = model.predict_proba(X_test)\n",
    "bmr = BayesMinimumRiskClassifier(calibration=False)\n",
    "pred_test = bmr.predict(prob_test, cost_matrix)\n",
    "print(classification_report(y_test, pred_test))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "cost_min_loss.append(loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) # transpose to align with slides\n",
    "\n",
    "print(\"costcla calibration on training set\")\n",
    "clf = RandomForestClassifier(random_state=0, n_estimators=100)\n",
    "model = clf.fit(X_train, y_train)\n",
    "prob_train = model.predict_proba(X_train)\n",
    "bmr = BayesMinimumRiskClassifier(calibration=True)\n",
    "bmr.fit(y_train, prob_train)\n",
    "prob_test = model.predict_proba(X_test)\n",
    "pred_test = bmr.predict(prob_test, cost_matrix)\n",
    "print(classification_report(y_test, pred_test))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "cost_min_loss.append(loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) # transpose to align with slides\n",
    "\n",
    "print(\"\\nsigmoid calibration\")\n",
    "clf = RandomForestClassifier(random_state=0, n_estimators=100)\n",
    "cc = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=3)\n",
    "model = cc.fit(X_train, y_train)\n",
    "prob_test = model.predict_proba(X_test)\n",
    "bmr = BayesMinimumRiskClassifier(calibration=False)\n",
    "pred_test = bmr.predict(prob_test, cost_matrix)\n",
    "print(classification_report(y_test, pred_test))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "cost_min_loss.append(loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) # transpose to align with slides\n",
    "\n",
    "print(\"\\nisotonic calibration\")\n",
    "clf = RandomForestClassifier(random_state=0, n_estimators=100)\n",
    "cc = CalibratedClassifierCV(clf, method=\"isotonic\", cv=3)\n",
    "model = cc.fit(X_train, y_train)\n",
    "prob_test = model.predict_proba(X_test)\n",
    "bmr = BayesMinimumRiskClassifier(calibration=False)\n",
    "pred_test = bmr.predict(prob_test, cost_matrix)\n",
    "print(classification_report(y_test, pred_test))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "cost_min_loss.append(loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) # transpose to align with slides\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14766ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without sampling\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-4a6f8f010c8b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0mclf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mRandomForestClassifier\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn_estimators\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m100\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrandom_state\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"without sampling\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mCounter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m \u001B[0;31m#0: 149, 1: 249\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from costcla.metrics import cost_loss\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "print(\"without sampling\")\n",
    "print(Counter(y_train))\n",
    "#0: 149, 1: 249\n",
    "\n",
    "model = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred).T) # transpose to align with slides\n",
    "loss = cost_loss(y_test, y_pred, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "\n",
    "# print(\"with undersampling\")\n",
    "# sampler = RandomUnderSampler(sampling_strategy={0: 149, 1: 37}, random_state=1)\n",
    "# X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "# print(Counter(y_rs))\n",
    "\n",
    "# model = clf.fit(X_rs, y_rs)\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
    "# print(confusion_matrix(y_test, y_pred).T) # transpose to align with slides\n",
    "# loss = cost_loss(y_test, y_pred, cost_matrix)\n",
    "# print(\"%d\\n\" %loss)\n",
    "\n",
    "# print(\"with oversampling\")\n",
    "# sampler = RandomOverSampler(sampling_strategy={0: 1000, 1: 249}, random_state=1)\n",
    "# X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "# print(Counter(y_rs))\n",
    "\n",
    "# model = clf.fit(X_rs, y_rs)\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
    "# print(confusion_matrix(y_test, y_pred).T) # transpose to align with slides\n",
    "# loss = cost_loss(y_test, y_pred, cost_matrix)\n",
    "# print(\"%d\\n\" %loss)\n",
    "\n",
    "\n",
    "count_y = Counter(y_train)\n",
    "major_class = count_y[0]\n",
    "minor_class = count_y[1]\n",
    "\n",
    "c = [1, 4]\n",
    "cost_major = int(major_class/c[1])\n",
    "cost_minor = int(minor_class/c[0])\n",
    "\n",
    "\n",
    "print(\"with combination\")\n",
    "sampler = RandomUnderSampler(sampling_strategy={0: cost_major, 1: minor_class}, random_state=1)\n",
    "X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "sampler = RandomOverSampler(sampling_strategy={0: cost_major, 1: cost_minor}, random_state=1)\n",
    "X_rs, y_rs = sampler.fit_resample(X_rs, y_rs)\n",
    "print(Counter(y_rs))\n",
    "\n",
    "model = clf.fit(X_rs, y_rs)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred).T) # transpose to align with slides\n",
    "loss = cost_loss(y_test, y_pred, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a05fc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without weights\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95       414\n",
      "           1       0.92      0.74      0.82       118\n",
      "\n",
      "    accuracy                           0.93       532\n",
      "   macro avg       0.92      0.86      0.89       532\n",
      "weighted avg       0.93      0.93      0.92       532\n",
      "\n",
      "132\n",
      "\n",
      "[[406  31]\n",
      " [  8  87]]\n",
      "\n",
      "with weights\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95       414\n",
      "           1       0.89      0.71      0.79       118\n",
      "\n",
      "    accuracy                           0.92       532\n",
      "   macro avg       0.91      0.84      0.87       532\n",
      "weighted avg       0.92      0.92      0.91       532\n",
      "\n",
      "146\n",
      "\n",
      "[[404  34]\n",
      " [ 10  84]]\n",
      "\n",
      "with weights (alternative)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95       414\n",
      "           1       0.89      0.71      0.79       118\n",
      "\n",
      "    accuracy                           0.92       532\n",
      "   macro avg       0.91      0.84      0.87       532\n",
      "weighted avg       0.92      0.92      0.91       532\n",
      "\n",
      "146\n",
      "\n",
      "[[404  34]\n",
      " [ 10  84]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from costcla.metrics import cost_loss\n",
    "\n",
    "data = pd.read_csv(\"data/fetal_health.csv\")\n",
    "X_train, X_test, y_train, y_test = cost_sensitive_learning.pre_processing_binary(data)\n",
    "\n",
    "print(\"without weights\")\n",
    "clf = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "#clf = SVC(kernel='linear', probability=False, C=1)\n",
    "#clf = DecisionTreeClassifier()\n",
    "model = clf.fit(X_train, y_train)\n",
    "pred_test = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred_test))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) # transpose to align with slides\n",
    "\n",
    "print(\"\\nwith weights\")\n",
    "# now create the sample weights according to y\n",
    "weights = np.zeros(y_train.shape[0])\n",
    "weights[np.where(y_train == 1)] = 4;\n",
    "weights[np.where(y_train == 0)] = 1;\n",
    "\n",
    "\n",
    "model = clf.fit(X_train, y_train, weights)\n",
    "pred_test = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred_test))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) # transpose to align with slides\n",
    "\n",
    "print(\"\\nwith weights (alternative)\")\n",
    "clf = RandomForestClassifier(n_estimators=10, random_state=0, class_weight={0: 1, 1: 4})\n",
    "model = clf.fit(X_train, y_train)\n",
    "pred_test = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred_test))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) # transpose to align with slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee249e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}